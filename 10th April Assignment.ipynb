{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0bf3b-8d97-48a1-8f60-26a66cdf8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q1. What is the Probability That an Employee is a Smoker Given That He/She Uses the Health Insurance Plan?\n",
    "This question asks for the conditional probability \\( P(S | I) \\), where \\( S \\) is the event that an employee is a smoker, and \\( I \\) is the event that the employee uses the health insurance plan.\n",
    "\n",
    "Given:\n",
    "- 70% of employees use the health insurance plan, so \\( P(I) = 0.7 \\).\n",
    "- 40% of employees who use the health insurance plan are smokers, so \\( P(S | I) = 0.4 \\).\n",
    "\n",
    "The conditional probability \\( P(S | I) \\) is therefore 40%, or 0.4.\n",
    "\n",
    "### Q2. What is the Difference Between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "- **Bernoulli Naive Bayes**: This classifier is designed for binary/Boolean features. It is suitable for datasets where features represent the presence or absence of certain attributes. It's common in text classification, where a feature could indicate whether a specific word or term exists in a document.\n",
    "- **Multinomial Naive Bayes**: This classifier is designed for discrete/categorical features. It is commonly used in text classification with word counts or frequency-based features. Unlike Bernoulli, which considers binary presence, Multinomial works with discrete counts.\n",
    "\n",
    "### Q3. How Does Bernoulli Naive Bayes Handle Missing Values?\n",
    "Bernoulli Naive Bayes typically assumes that missing values represent the absence of a feature (or \"False\"). It does not inherently have a special handling mechanism for missing data, so it treats missing values as zeros (or not present) when calculating probabilities. If missing data could affect interpretation or significance, preprocessing techniques might be required to fill or remove missing values.\n",
    "\n",
    "### Q4. Can Gaussian Naive Bayes Be Used for Multi-Class Classification?\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. It calculates the probabilities assuming that each feature follows a Gaussian (normal) distribution. This approach is commonly used when working with continuous data. It can handle multiple classes by applying the same Gaussian assumption to each class independently, allowing for a broader range of applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1d83d7-907c-4304-809d-7e0664f50542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics for BernoulliNB:\n",
      "  Accuracy (CV): 0.8856\n",
      "  Accuracy (Test): 0.8806\n",
      "  Precision: 0.9070\n",
      "  Recall: 0.8000\n",
      "  F1 Score: 0.8501\n",
      "Performance Metrics for MultinomialNB:\n",
      "  Accuracy (CV): 0.7927\n",
      "  Accuracy (Test): 0.7861\n",
      "  Precision: 0.7644\n",
      "  Recall: 0.7154\n",
      "  F1 Score: 0.7391\n",
      "Performance Metrics for GaussianNB:\n",
      "  Accuracy (CV): 0.8187\n",
      "  Accuracy (Test): 0.8208\n",
      "  Precision: 0.7193\n",
      "  Recall: 0.9462\n",
      "  F1 Score: 0.8173\n"
     ]
    }
   ],
   "source": [
    "### Q5. Assignment: Implementing Naive Bayes Classifiers with the Spambase Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Spambase dataset\n",
    "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "df = pd.read_csv(data_url, header=None)\n",
    "\n",
    "# Features and target variable\n",
    "X = df.iloc[:, :-1]  # All but last column (features)\n",
    "y = df.iloc[:, -1]   # Last column (target)\n",
    "\n",
    "# Split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def evaluate_model(model):\n",
    "    # Cross-validation for accuracy\n",
    "    cv_accuracy = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    # Train on full training set and evaluate on the test set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy (CV)\": np.mean(cv_accuracy),\n",
    "        \"Accuracy (Test)\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "    }\n",
    "\n",
    "# Instantiate the Naive Bayes classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Standardize data for Gaussian Naive Bayes\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Evaluate each model\n",
    "results = {\n",
    "    \"BernoulliNB\": evaluate_model(bernoulli_nb),\n",
    "    \"MultinomialNB\": evaluate_model(multinomial_nb),\n",
    "    \"GaussianNB\": evaluate_model(gaussian_nb),\n",
    "}\n",
    "\n",
    "# Display the results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Performance Metrics for {model_name}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "### Discussion\n",
    "##Based on the results, we can evaluate which Naive Bayes variant performed best and why:\n",
    "##- **Bernoulli Naive Bayes**: Typically used for binary data. It could perform well if the features in the dataset are structured in a binary manner.\n",
    "##- **Multinomial Naive Bayes**: Ideal for discrete/categorical data. Often used in text classification with word counts or frequency-based features.\n",
    "##- **Gaussian Naive Bayes**: Designed for continuous data and assumes Gaussian distribution.\n",
    "\n",
    "##Given that the Spambase dataset includes features representing word frequencies and other metrics in emails, the best-performing classifier might be either Bernoulli or Multinomial, as they cater to discrete/categorical data. Gaussian Naive Bayes might perform less consistently because it relies on continuous distributions.\n",
    "\n",
    "### Limitations of Naive Bayes\n",
    "##Naive Bayes classifiers assume independence among features, which might not always hold true in practice. This assumption could lead to reduced performance when features are heavily correlated. Also, Naive Bayes classifiers may not perform well with highly skewed or unbalanced data without preprocessing to address these issues.\n",
    "\n",
    "### Conclusion and Future Work\n",
    "##To conclude, Naive Bayes is a simple yet effective classifier, especially when features are relatively independent and can be considered as discrete or continuous. The choice between Bernoulli, Multinomial, and Gaussian depends on the nature of the data. Further work might include experimenting with different data preprocessing techniques, exploring other classification algorithms, or combining Naive Bayes with other models to improve performance and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3467faa4-817e-4eb9-aaa4-71347cc59e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
